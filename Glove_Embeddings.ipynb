{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Glove_Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XmWHI76Dq5ch",
        "gMis6YlKrf7S",
        "x26LuCqjuf_8"
      ],
      "toc_visible": true,
      "mount_file_id": "1VRSymHOQynhsdccW6zfqOMpe1-Rqvjsw",
      "authorship_tag": "ABX9TyOrXbtE0uvxRRNt3S1kaIgq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajagopal17/spacy-notebooks/blob/master/Glove_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UkzOZPDeUd",
        "colab_type": "text"
      },
      "source": [
        "### https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnS3RpBl8VXn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1b18cbc3-5aed-471b-a547-e6962398103b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.rc('font', size=14)\n",
        "sns.set(style='white')\n",
        "sns.set(style='whitegrid', color_codes=True)\n",
        "import csv\n",
        "import re\n",
        "from __future__ import unicode_literals\n",
        "import spacy\n",
        "from spacy.tokens import doc\n",
        "nlp=spacy.load('en')\n",
        "import en_core_web_sm\n",
        "#nlp=en_core_web_md.load()\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "parser = English()\n",
        "import string\n",
        "punctuations=string.punctuation\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwzZ-GQu84k-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1ce36db0-6f7d-4130-92ac-2c951f09bfb5"
      },
      "source": [
        "!git clone https://github.com/MohammadWasil/Sentiment-Analysis-IMDb-Movie-Review.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Sentiment-Analysis-IMDb-Movie-Review'...\n",
            "remote: Enumerating objects: 70, done.\u001b[K\n",
            "remote: Total 70 (delta 0), reused 0 (delta 0), pack-reused 70\u001b[K\n",
            "Unpacking objects: 100% (70/70), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IzkCmME8uLZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4fc44747-eea6-4257-d291-784cf331cd5f"
      },
      "source": [
        "data_train =pd.read_csv('/content/Sentiment-Analysis-IMDb-Movie-Review/labeledTrainData.tsv', delimiter='\\t',encoding=\"utf-8\")\n",
        "#data_test =('/content/Sentiment-Analysis-IMDb-Movie-Review/testData.tsv')\n",
        "data_file = data_train[['sentiment','review']].copy()\n",
        "train_ds = data_file.head(15000).copy()\n",
        "test_ds  = data_file.tail(5000).copy()\n",
        "train_ds.to_csv('/content/train_ds.csv',index=False)\n",
        "test_ds.to_csv('/content/test_ds.csv',index=False)\n",
        "train_ds.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>With all this stuff going down at the moment w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>It must be assumed that those who praised this...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                             review\n",
              "0          1  With all this stuff going down at the moment w...\n",
              "1          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
              "2          0  The film starts with a manager (Nicholas Bell)...\n",
              "3          0  It must be assumed that those who praised this...\n",
              "4          1  Superbly trashy and wondrously unpretentious 8..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmWHI76Dq5ch",
        "colab_type": "text"
      },
      "source": [
        "# Convert column to string, apply nlp pipe lines for lemmatization,ents, etc\n",
        "  \n",
        "\n",
        "1.   Convert column to list and feed it back to new data frame\n",
        "2.   Get frequency and plot on graph\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl5nJ0sX-Npx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert each row of reviews column to string and store it in a file\n",
        "\n",
        "temp_file=data_train['Reviews'].apply(lambda x:str(x))\n",
        "temp_file=data_train['Reviews'].apply(lambda x:re.sub('[^A-Z 0-9 a-z-]+','',x))\n",
        "temp_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH25cwkF-9me",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemma_list=list([token.lemma_ for token in doc if token.is_stop==False] for doc in nlp.pipe(temp_file, n_threads=2,batch_size=1000,disable=['tagger','parser','ner']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P96LPwrBDqKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemma_names=list([token.text for token in doc if token.pos_=='PROPN'] for doc in nlp.pipe(temp_file,batch_size=1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np7l1SV-Xtnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemma_person=list([token.text for token in doc if token.ent_type_=='PERSON'] for doc in nlp.pipe(temp_file,batch_size=1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxgwmrm9Gm50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_names=[]\n",
        "for x in lemma_person:\n",
        "  for y in x:\n",
        "    get_names.append(y)\n",
        "get_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJmTYGofHgsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_clean['lemma']=lemma_list\n",
        "data_clean['Reviews']=lemma_names\n",
        "data_clean['names']=lemma_person"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUzNTmpjH3s8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_clean.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loZ92U2uNIv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "word_freq=Counter(get_names)\n",
        "word_freq\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQPzLY4lOzoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_freq_graph =pd.DataFrame(list(word_freq.items()),columns=['name','freq'])\n",
        "word_freq_graph\n",
        "final_df=word_freq_graph[word_freq_graph['freq']>800 ]\n",
        "final_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55Xj64CRQ4rt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plot\n",
        "final_df.plot.barh(x='name', y='freq', title=\"Frequency of the mention of lead character's name\");\n",
        "\n",
        "plot.show(block=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMis6YlKrf7S",
        "colab_type": "text"
      },
      "source": [
        "# Torch Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNWfhL6srjmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import Field,TabularDataset,BucketIterator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk2CUAZs1b5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_train,X_test,y_train,y_test = train_test_split('/content/Sentiment-Analysis-IMDb-Movie-Review/labeledTrainData.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8IoZ2OE6Yc9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f0edc4-a506-41e7-a8de-0af7feaa9772"
      },
      "source": [
        "!git clone https://github.com/AladdinPerzon/Machine-Learning-Collection.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Machine-Learning-Collection'...\n",
            "remote: Enumerating objects: 9466, done.\u001b[K\n",
            "remote: Total 9466 (delta 0), reused 0 (delta 0), pack-reused 9466\u001b[K\n",
            "Receiving objects: 100% (9466/9466), 1.07 GiB | 39.99 MiB/s, done.\n",
            "Resolving deltas: 100% (661/661), done.\n",
            "Checking out files: 100% (154/154), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKUy_sPaY42G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(text):\n",
        "  return[token.text for token in nlp(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sff1UpVqsUEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokenize= lambda x: x.split()\n",
        "TEXT  = Field(sequential= True,use_vocab=True,tokenize=tokenize,lower=True)\n",
        "LABEL = Field(sequential=False,use_vocab=False)\n",
        "fields={'sentiment':LABEL,'review':TEXT}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x26LuCqjuf_8",
        "colab_type": "text"
      },
      "source": [
        "#Split dataset for train & test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypDgBQ3tuhEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_data, test_data= TabularDataset(path='/content/sample_data/test_d.csv',format='csv',fields={'sentiment':sentiment,'review':review})\n",
        "train = TabularDataset(path='/content/sample_data/train_ds.csv', \n",
        "                        format='csv', \n",
        "                        fields=[(\"sentiment\",LABEL),\n",
        "                                (\"review\",TEXT)],  \n",
        "                        skip_header=True)\n",
        "test = TabularDataset(path='/content/sample_data/test_ds.csv', \n",
        "                        format='csv', \n",
        "                        fields= [(\"sentiment\",LABEL),\n",
        "                                (\"review\",TEXT)],  \n",
        "                        skip_header=True)                             \n",
        "                       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilvVd5MMUEeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT.build_vocab(train,max_size=10000,min_freq=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbTAQIWQXnWu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a4f4f86-a15e-4c32-b188-fda95a80d53f"
      },
      "source": [
        "TEXT.vocab.itos\n",
        "\n",
        "#TEXT.vocab.stoi['movie']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>',\n",
              " '<pad>',\n",
              " 'the',\n",
              " ',',\n",
              " '.',\n",
              " 'and',\n",
              " 'a',\n",
              " 'of',\n",
              " 'to',\n",
              " 'is',\n",
              " 'in',\n",
              " 'it',\n",
              " 'i',\n",
              " 'that',\n",
              " 'this',\n",
              " \"'s\",\n",
              " '-',\n",
              " 'was',\n",
              " '/><br',\n",
              " 'as',\n",
              " 'with',\n",
              " '\"',\n",
              " 'for',\n",
              " 'but',\n",
              " 'movie',\n",
              " 'film',\n",
              " ')',\n",
              " 'on',\n",
              " '(',\n",
              " 'you',\n",
              " 'are',\n",
              " \"n't\",\n",
              " 'not',\n",
              " 'his',\n",
              " 'he',\n",
              " 'have',\n",
              " 'be',\n",
              " 'one',\n",
              " 'all',\n",
              " 'at',\n",
              " 'by',\n",
              " 'an',\n",
              " 'who',\n",
              " 'they',\n",
              " 'from',\n",
              " 'like',\n",
              " 'her',\n",
              " '!',\n",
              " 'so',\n",
              " \"'\",\n",
              " 'about',\n",
              " 'there',\n",
              " 'or',\n",
              " 'has',\n",
              " 'just',\n",
              " 'out',\n",
              " 'good',\n",
              " 'some',\n",
              " 'do',\n",
              " 'if',\n",
              " 'what',\n",
              " 'she',\n",
              " 'more',\n",
              " 'very',\n",
              " 'when',\n",
              " '?',\n",
              " 'up',\n",
              " 'would',\n",
              " 'no',\n",
              " 'even',\n",
              " 'their',\n",
              " 'time',\n",
              " 'my',\n",
              " 'can',\n",
              " 'which',\n",
              " 'only',\n",
              " 'had',\n",
              " 'really',\n",
              " 'were',\n",
              " 'story',\n",
              " 'did',\n",
              " 'see',\n",
              " 'people',\n",
              " ':',\n",
              " 'me',\n",
              " 'does',\n",
              " 'than',\n",
              " 'we',\n",
              " 'could',\n",
              " 'well',\n",
              " 'been',\n",
              " 'will',\n",
              " 'get',\n",
              " 'into',\n",
              " 'most',\n",
              " 'much',\n",
              " 'great',\n",
              " 'other',\n",
              " 'because',\n",
              " 'him',\n",
              " 'how',\n",
              " 'first',\n",
              " '...',\n",
              " 'bad',\n",
              " 'also',\n",
              " 'then',\n",
              " 'them',\n",
              " 'its',\n",
              " 'made',\n",
              " 'make',\n",
              " 'any',\n",
              " 'way',\n",
              " 'after',\n",
              " 'too',\n",
              " '/>the',\n",
              " 'movies',\n",
              " 'think',\n",
              " 'characters',\n",
              " '<',\n",
              " 'br',\n",
              " 'films',\n",
              " 'many',\n",
              " 'show',\n",
              " 'being',\n",
              " 'character',\n",
              " 'watch',\n",
              " 'two',\n",
              " 'off',\n",
              " 'acting',\n",
              " 'over',\n",
              " 'man',\n",
              " 'never',\n",
              " 'plot',\n",
              " 'love',\n",
              " 'life',\n",
              " ';',\n",
              " 'ever',\n",
              " 'seen',\n",
              " 'little',\n",
              " 'while',\n",
              " 'best',\n",
              " 'where',\n",
              " 'know',\n",
              " 'here',\n",
              " 'your',\n",
              " 'say',\n",
              " 'end',\n",
              " 'these',\n",
              " 'such',\n",
              " 'should',\n",
              " 'something',\n",
              " 'through',\n",
              " 'those',\n",
              " 'years',\n",
              " 'back',\n",
              " 'scene',\n",
              " 'better',\n",
              " \"'ve\",\n",
              " 'why',\n",
              " 'scenes',\n",
              " 'still',\n",
              " 'going',\n",
              " '/',\n",
              " 'go',\n",
              " '&',\n",
              " 'though',\n",
              " '*',\n",
              " 'actors',\n",
              " 'old',\n",
              " \"'m\",\n",
              " 'director',\n",
              " 'watching',\n",
              " 'thing',\n",
              " 'another',\n",
              " 'few',\n",
              " 'now',\n",
              " \"'re\",\n",
              " 'work',\n",
              " 'makes',\n",
              " 'things',\n",
              " 'find',\n",
              " 'real',\n",
              " 'actually',\n",
              " 'nothing',\n",
              " 'before',\n",
              " 'lot',\n",
              " 'same',\n",
              " 'again',\n",
              " 'look',\n",
              " '--',\n",
              " 'new',\n",
              " 'part',\n",
              " 'between',\n",
              " 'want',\n",
              " 'down',\n",
              " 'quite',\n",
              " '/>i',\n",
              " 'every',\n",
              " 'got',\n",
              " 'young',\n",
              " 'action',\n",
              " 'us',\n",
              " 'right',\n",
              " 'cast',\n",
              " 'horror',\n",
              " 'pretty',\n",
              " 'fact',\n",
              " 'funny',\n",
              " 'big',\n",
              " 'however',\n",
              " 'always',\n",
              " 'thought',\n",
              " 'times',\n",
              " 'almost',\n",
              " 'interesting',\n",
              " 'take',\n",
              " 'world',\n",
              " 'ca',\n",
              " 'come',\n",
              " 'give',\n",
              " 'saw',\n",
              " 'guy',\n",
              " 'last',\n",
              " 'gets',\n",
              " 'own',\n",
              " 'worst',\n",
              " 'long',\n",
              " 'without',\n",
              " 'must',\n",
              " 'may',\n",
              " 'rather',\n",
              " 'role',\n",
              " 'whole',\n",
              " 'making',\n",
              " 'around',\n",
              " 'enough',\n",
              " 'far',\n",
              " 'minutes',\n",
              " 'am',\n",
              " 'both',\n",
              " 'music',\n",
              " 'seems',\n",
              " 'might',\n",
              " 'comedy',\n",
              " 'least',\n",
              " 'script',\n",
              " 'family',\n",
              " 'point',\n",
              " 'original',\n",
              " 'course',\n",
              " 'found',\n",
              " 'kind',\n",
              " 'day',\n",
              " 'book',\n",
              " 'hard',\n",
              " 'since',\n",
              " 'woman',\n",
              " 'tv',\n",
              " 'anything',\n",
              " 'fun',\n",
              " 'comes',\n",
              " 'performance',\n",
              " 'series',\n",
              " 'play',\n",
              " 'probably',\n",
              " 'away',\n",
              " 'feel',\n",
              " 'american',\n",
              " 'ending',\n",
              " 'having',\n",
              " 'money',\n",
              " 'played',\n",
              " 'looking',\n",
              " 'our',\n",
              " 'believe',\n",
              " 'done',\n",
              " 'bit',\n",
              " \"'ll\",\n",
              " 'father',\n",
              " 'goes',\n",
              " 'sure',\n",
              " 'anyone',\n",
              " 'different',\n",
              " 'three',\n",
              " 'reason',\n",
              " 'together',\n",
              " 'especially',\n",
              " 'trying',\n",
              " 'yet',\n",
              " 'dvd',\n",
              " 'someone',\n",
              " 'let',\n",
              " 'war',\n",
              " 'girl',\n",
              " 'worth',\n",
              " 'although',\n",
              " 'set',\n",
              " 'each',\n",
              " 'job',\n",
              " 'put',\n",
              " 'version',\n",
              " 'once',\n",
              " 'screen',\n",
              " 'true',\n",
              " '/>this',\n",
              " 'house',\n",
              " 'john',\n",
              " 'looks',\n",
              " 'plays',\n",
              " \"'d\",\n",
              " 'actor',\n",
              " 'everyone',\n",
              " 'place',\n",
              " 'beautiful',\n",
              " 'main',\n",
              " 'else',\n",
              " 'high',\n",
              " 'sense',\n",
              " 'watched',\n",
              " 'during',\n",
              " 'excellent',\n",
              " 'night',\n",
              " 'read',\n",
              " 'poor',\n",
              " 'year',\n",
              " 'completely',\n",
              " 'men',\n",
              " 'truly',\n",
              " 'less',\n",
              " 'seem',\n",
              " 'everything',\n",
              " 'given',\n",
              " 'shows',\n",
              " 'use',\n",
              " '\\\\\"the',\n",
              " 'line',\n",
              " 'women',\n",
              " 'maybe',\n",
              " 'said',\n",
              " 'seeing',\n",
              " 'takes',\n",
              " 'effects',\n",
              " 'left',\n",
              " 'idea',\n",
              " 'black',\n",
              " 'production',\n",
              " 'against',\n",
              " 'wife',\n",
              " 'instead',\n",
              " 'later',\n",
              " 'nice',\n",
              " 'simply',\n",
              " 'audience',\n",
              " 'small',\n",
              " 'star',\n",
              " 'video',\n",
              " 'came',\n",
              " 'next',\n",
              " 'shot',\n",
              " 'wrong',\n",
              " '10',\n",
              " 'classic',\n",
              " 'couple',\n",
              " 'fan',\n",
              " 'special',\n",
              " 'used',\n",
              " 'along',\n",
              " 'second',\n",
              " '2',\n",
              " 'tell',\n",
              " 'top',\n",
              " 'death',\n",
              " 'enjoy',\n",
              " 'rest',\n",
              " 'short',\n",
              " 'friends',\n",
              " 'keep',\n",
              " 'understand',\n",
              " 'definitely',\n",
              " 'full',\n",
              " 'start',\n",
              " 'camera',\n",
              " 'doing',\n",
              " 'human',\n",
              " 'worse',\n",
              " 'children',\n",
              " 'dead',\n",
              " 'lines',\n",
              " 'try',\n",
              " 'called',\n",
              " 'episode',\n",
              " 'low',\n",
              " 'need',\n",
              " 'half',\n",
              " 'hollywood',\n",
              " 'live',\n",
              " 'often',\n",
              " 'playing',\n",
              " 'remember',\n",
              " 'become',\n",
              " 'boring',\n",
              " 'face',\n",
              " 'mind',\n",
              " 'person',\n",
              " 'until',\n",
              " 'written',\n",
              " 'entertaining',\n",
              " '3',\n",
              " 'early',\n",
              " 'himself',\n",
              " 'moments',\n",
              " 'performances',\n",
              " 'piece',\n",
              " 'supposed',\n",
              " 'awful',\n",
              " 'sort',\n",
              " 'style',\n",
              " 'certainly',\n",
              " 'home',\n",
              " 'name',\n",
              " '\\x96',\n",
              " 'case',\n",
              " 'sex',\n",
              " '/>it',\n",
              " 'enjoyed',\n",
              " 'loved',\n",
              " 'recommend',\n",
              " 'sometimes',\n",
              " 'stars',\n",
              " 'went',\n",
              " '<br',\n",
              " 'boy',\n",
              " 'dark',\n",
              " 'hope',\n",
              " 'liked',\n",
              " 'totally',\n",
              " 'friend',\n",
              " 'perfect',\n",
              " 'budget',\n",
              " 'felt',\n",
              " 'getting',\n",
              " 'picture',\n",
              " 'school',\n",
              " 'wonderful',\n",
              " 'beginning',\n",
              " 'despite',\n",
              " 'itself',\n",
              " 'either',\n",
              " 'fans',\n",
              " 'help',\n",
              " 'kids',\n",
              " 'perhaps',\n",
              " '....',\n",
              " 'art',\n",
              " 'city',\n",
              " 'dialogue',\n",
              " 'mean',\n",
              " 'mother',\n",
              " 'others',\n",
              " 'seemed',\n",
              " 'stories',\n",
              " 'stupid',\n",
              " 'white',\n",
              " 'actress',\n",
              " 'finally',\n",
              " 'fine',\n",
              " 'several',\n",
              " 'waste',\n",
              " 'absolutely',\n",
              " 'gives',\n",
              " 'flick',\n",
              " 'head',\n",
              " 'title',\n",
              " 'able',\n",
              " 'care',\n",
              " 'drama',\n",
              " 'heard',\n",
              " 'lead',\n",
              " 'lost',\n",
              " 'sound',\n",
              " 'guess',\n",
              " 'brilliant',\n",
              " 'cinema',\n",
              " 'directed',\n",
              " 'genre',\n",
              " 'guys',\n",
              " 'humor',\n",
              " 'quality',\n",
              " 'child',\n",
              " 'tries',\n",
              " 'whose',\n",
              " 'lives',\n",
              " 'past',\n",
              " 'terrible',\n",
              " 'turn',\n",
              " 'yes',\n",
              " '..',\n",
              " 'becomes',\n",
              " 'car',\n",
              " 'direction',\n",
              " 'entire',\n",
              " 'example',\n",
              " 'daughter',\n",
              " 'english',\n",
              " 'feeling',\n",
              " 'themselves',\n",
              " 'class',\n",
              " 'country',\n",
              " 'etc',\n",
              " 'evil',\n",
              " 'eyes',\n",
              " 'gore',\n",
              " 'horrible',\n",
              " 'living',\n",
              " 'myself',\n",
              " 'turns',\n",
              " 'based',\n",
              " 'comic',\n",
              " 'known',\n",
              " 'ago',\n",
              " 'brother',\n",
              " 'fight',\n",
              " 'running',\n",
              " 'soon',\n",
              " 'taking',\n",
              " 'today',\n",
              " 'amazing',\n",
              " 'british',\n",
              " 'ends',\n",
              " 'none',\n",
              " 'parts',\n",
              " 'starts',\n",
              " 'wo',\n",
              " 'writer',\n",
              " '1',\n",
              " 'favorite',\n",
              " 'late',\n",
              " 'particularly',\n",
              " 'problem',\n",
              " 'town',\n",
              " 'working',\n",
              " 'works',\n",
              " 'already',\n",
              " 'days',\n",
              " 'expect',\n",
              " 'gave',\n",
              " 'group',\n",
              " 'hero',\n",
              " 'interest',\n",
              " 'lee',\n",
              " 'looked',\n",
              " 'police',\n",
              " 'roles',\n",
              " 'serious',\n",
              " 'side',\n",
              " 'son',\n",
              " 'wanted',\n",
              " 'close',\n",
              " 'david',\n",
              " 'final',\n",
              " 'girls',\n",
              " 'history',\n",
              " 'involved',\n",
              " 'killer',\n",
              " 'age',\n",
              " 'happens',\n",
              " 'mostly',\n",
              " 'paul',\n",
              " 'self',\n",
              " 'strong',\n",
              " 'usually',\n",
              " 'voice',\n",
              " 'behind',\n",
              " 'change',\n",
              " 'experience',\n",
              " 'hand',\n",
              " 'happened',\n",
              " 'kill',\n",
              " 'run',\n",
              " 'view',\n",
              " 'zombie',\n",
              " 'attempt',\n",
              " 'body',\n",
              " 'opening',\n",
              " 'usual',\n",
              " 'within',\n",
              " 'documentary',\n",
              " 'except',\n",
              " 'god',\n",
              " 'heart',\n",
              " 'highly',\n",
              " 'hit',\n",
              " 'husband',\n",
              " 'kid',\n",
              " 'king',\n",
              " 'level',\n",
              " 'match',\n",
              " 'peter',\n",
              " 'took',\n",
              " 'cop',\n",
              " 'cut',\n",
              " 'entertainment',\n",
              " 'started',\n",
              " 'talk',\n",
              " 'turned',\n",
              " 'viewer',\n",
              " 'writing',\n",
              " 'coming',\n",
              " 'difficult',\n",
              " 'feels',\n",
              " 'happen',\n",
              " 'lack',\n",
              " 'laugh',\n",
              " 'number',\n",
              " 'order',\n",
              " 'overall',\n",
              " 'political',\n",
              " 'song',\n",
              " 'thinking',\n",
              " 'told',\n",
              " 'under',\n",
              " '/>there',\n",
              " 'across',\n",
              " 'clearly',\n",
              " 'five',\n",
              " 'including',\n",
              " 'light',\n",
              " 'lots',\n",
              " 'michael',\n",
              " 'stop',\n",
              " 'theme',\n",
              " 'wonder',\n",
              " '/>in',\n",
              " 'act',\n",
              " 'brought',\n",
              " 'cinematography',\n",
              " 'complete',\n",
              " 'disappointed',\n",
              " 'finds',\n",
              " 'general',\n",
              " 'jokes',\n",
              " 'slow',\n",
              " 'tells',\n",
              " 'throughout',\n",
              " 'apparently',\n",
              " 'chance',\n",
              " 'important',\n",
              " 'local',\n",
              " 'oh',\n",
              " 'save',\n",
              " 'career',\n",
              " 'cheap',\n",
              " 'famous',\n",
              " 'fast',\n",
              " 'female',\n",
              " 'hilarious',\n",
              " 'killed',\n",
              " 'leave',\n",
              " 'matter',\n",
              " 'sad',\n",
              " 'stage',\n",
              " 'type',\n",
              " 'wants',\n",
              " '/>if',\n",
              " 'dance',\n",
              " 'fantastic',\n",
              " 'happy',\n",
              " 'major',\n",
              " 'moment',\n",
              " 'obvious',\n",
              " 'ones',\n",
              " 'songs',\n",
              " 'unfortunately',\n",
              " '$',\n",
              " 'due',\n",
              " 'imagine',\n",
              " 'modern',\n",
              " 'musical',\n",
              " 'non',\n",
              " 'oscar',\n",
              " 'released',\n",
              " 'ridiculous',\n",
              " 'sexual',\n",
              " 'superb',\n",
              " 'bunch',\n",
              " 'herself',\n",
              " 'obviously',\n",
              " 'opinion',\n",
              " 'sequence',\n",
              " 'upon',\n",
              " 'using',\n",
              " 'word',\n",
              " 'alone',\n",
              " 'b',\n",
              " 'blood',\n",
              " 'bring',\n",
              " 'decent',\n",
              " 'earth',\n",
              " 'french',\n",
              " 'george',\n",
              " 'saying',\n",
              " 'simple',\n",
              " 'single',\n",
              " 'stuff',\n",
              " 'taken',\n",
              " 'talking',\n",
              " 'thriller',\n",
              " 'yourself',\n",
              " 'begin',\n",
              " 'easily',\n",
              " 'easy',\n",
              " 'elements',\n",
              " 'enjoyable',\n",
              " 'filmed',\n",
              " 'four',\n",
              " 'game',\n",
              " 'hour',\n",
              " 'it.<br',\n",
              " 'novel',\n",
              " 'possible',\n",
              " 'rating',\n",
              " 'relationship',\n",
              " 'release',\n",
              " 'review',\n",
              " 'sets',\n",
              " 'somewhat',\n",
              " '/>but',\n",
              " 'annoying',\n",
              " 'buy',\n",
              " 'casting',\n",
              " 'forced',\n",
              " 'knows',\n",
              " 'middle',\n",
              " 'moving',\n",
              " 'nature',\n",
              " 'near',\n",
              " 'parents',\n",
              " 'power',\n",
              " 'richard',\n",
              " 'subject',\n",
              " 'typical',\n",
              " 'wish',\n",
              " 'words',\n",
              " '4',\n",
              " '\\\\the',\n",
              " 'above',\n",
              " 'baby',\n",
              " 'crap',\n",
              " 'crime',\n",
              " 'die',\n",
              " 'era',\n",
              " 'message',\n",
              " 'supporting',\n",
              " 'tale',\n",
              " 'team',\n",
              " 'beyond',\n",
              " 'certain',\n",
              " 'deal',\n",
              " 'eventually',\n",
              " 'exactly',\n",
              " 'eye',\n",
              " 'joe',\n",
              " 'lady',\n",
              " 'murder',\n",
              " 'ok',\n",
              " 'powerful',\n",
              " 'score',\n",
              " 'strange',\n",
              " 'talent',\n",
              " 'tom',\n",
              " 'york',\n",
              " 'anyway',\n",
              " 'avoid',\n",
              " 'episodes',\n",
              " 'future',\n",
              " 'romantic',\n",
              " 'tried',\n",
              " 'violence',\n",
              " 'ways',\n",
              " '5',\n",
              " 'events',\n",
              " 'extremely',\n",
              " 'knew',\n",
              " 'means',\n",
              " 'room',\n",
              " 'average',\n",
              " 'begins',\n",
              " 'cool',\n",
              " 'decided',\n",
              " 'doubt',\n",
              " 'figure',\n",
              " 'form',\n",
              " 'indeed',\n",
              " 'leads',\n",
              " 'miss',\n",
              " 'predictable',\n",
              " 'scary',\n",
              " 'sister',\n",
              " 'stand',\n",
              " 'ten',\n",
              " 'america',\n",
              " 'attention',\n",
              " 'basically',\n",
              " 'credits',\n",
              " 'falls',\n",
              " 'forget',\n",
              " 'jane',\n",
              " 'named',\n",
              " 'needed',\n",
              " 'points',\n",
              " 'question',\n",
              " 'stay',\n",
              " 'surprise',\n",
              " 'among',\n",
              " 'appear',\n",
              " 'ask',\n",
              " 'atmosphere',\n",
              " 'cute',\n",
              " 'earlier',\n",
              " 'fantasy',\n",
              " 'james',\n",
              " 'male',\n",
              " 'plenty',\n",
              " 'possibly',\n",
              " 'reality',\n",
              " 's',\n",
              " 'showing',\n",
              " 'silly',\n",
              " 'success',\n",
              " 'zombies',\n",
              " '/>a',\n",
              " '>',\n",
              " 'call',\n",
              " 'failed',\n",
              " 'feature',\n",
              " 'features',\n",
              " 'gay',\n",
              " 'giving',\n",
              " 'hours',\n",
              " 'huge',\n",
              " 'mention',\n",
              " 'mr.',\n",
              " 'period',\n",
              " 'sequel',\n",
              " 'seriously',\n",
              " 'weak',\n",
              " 'background',\n",
              " 'ben',\n",
              " 'comments',\n",
              " 'emotional',\n",
              " 'fall',\n",
              " 'flat',\n",
              " 'greatest',\n",
              " 'hell',\n",
              " 'jack',\n",
              " 'lame',\n",
              " 'mary',\n",
              " 'masterpiece',\n",
              " 'needs',\n",
              " 'sam',\n",
              " 'setting',\n",
              " 'straight',\n",
              " 'television',\n",
              " 'write',\n",
              " 'clear',\n",
              " 'fair',\n",
              " 'follow',\n",
              " 'hands',\n",
              " 'kept',\n",
              " 'leaves',\n",
              " 'list',\n",
              " 'result',\n",
              " 'says',\n",
              " 'soundtrack',\n",
              " 'state',\n",
              " 'adventure',\n",
              " 'appears',\n",
              " 'beauty',\n",
              " 'culture',\n",
              " 'directing',\n",
              " 'directors',\n",
              " 'edge',\n",
              " 'effort',\n",
              " 'footage',\n",
              " 'meet',\n",
              " 'particular',\n",
              " 'please',\n",
              " 'plus',\n",
              " 'robert',\n",
              " 'romance',\n",
              " 'slightly',\n",
              " 'surprised',\n",
              " 'truth',\n",
              " 'viewers',\n",
              " 'waiting',\n",
              " 'water',\n",
              " 'weird',\n",
              " 'whom',\n",
              " '/>and',\n",
              " '20',\n",
              " '\\\\',\n",
              " 'appreciate',\n",
              " 'cheesy',\n",
              " 'cold',\n",
              " 'editing',\n",
              " 'exciting',\n",
              " 'film.<br',\n",
              " 'hate',\n",
              " 'japanese',\n",
              " 'manages',\n",
              " 'move',\n",
              " 'moves',\n",
              " 'open',\n",
              " 'pay',\n",
              " 'rent',\n",
              " 'social',\n",
              " 'society',\n",
              " 'third',\n",
              " 'whether',\n",
              " 'actual',\n",
              " 'add',\n",
              " 'allen',\n",
              " 'attempts',\n",
              " 'co',\n",
              " 'comment',\n",
              " 'copy',\n",
              " 'de',\n",
              " 'fighting',\n",
              " 'forward',\n",
              " 'free',\n",
              " 'minute',\n",
              " 'older',\n",
              " 'poorly',\n",
              " 'public',\n",
              " 'tension',\n",
              " 'theater',\n",
              " 'trouble',\n",
              " 'viewing',\n",
              " 'won',\n",
              " 'animation',\n",
              " 'badly',\n",
              " 'dream',\n",
              " 'dull',\n",
              " 'ended',\n",
              " 'further',\n",
              " 'hear',\n",
              " 'hot',\n",
              " 'london',\n",
              " 'material',\n",
              " 'mess',\n",
              " 'nearly',\n",
              " 'nudity',\n",
              " 'perfectly',\n",
              " 'portrayed',\n",
              " 'situation',\n",
              " 'somehow',\n",
              " 'suspense',\n",
              " 'various',\n",
              " 'whatever',\n",
              " 'became',\n",
              " 'century',\n",
              " 'considering',\n",
              " 'filled',\n",
              " 'fire',\n",
              " 'learn',\n",
              " 'missing',\n",
              " 'nor',\n",
              " 'odd',\n",
              " 'otherwise',\n",
              " 'pure',\n",
              " 're',\n",
              " 'realistic',\n",
              " 'realize',\n",
              " 'reviews',\n",
              " 'store',\n",
              " 'street',\n",
              " 'telling',\n",
              " 'total',\n",
              " 'unique',\n",
              " 'unless',\n",
              " 'wait',\n",
              " 'western',\n",
              " '/>so',\n",
              " '30',\n",
              " 'apart',\n",
              " 'club',\n",
              " 'creepy',\n",
              " 'deserves',\n",
              " 'dramatic',\n",
              " 'dumb',\n",
              " 'expected',\n",
              " 'interested',\n",
              " 'knowing',\n",
              " 'mad',\n",
              " 'missed',\n",
              " 'note',\n",
              " 'office',\n",
              " 'recommended',\n",
              " 'screenplay',\n",
              " 'sit',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OGzS9g_jn4X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "aea2ce72-64a0-4b7e-ca28-ac7071ca0bd6"
      },
      "source": [
        "train_ds.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>With all this stuff going down at the moment w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>It must be assumed that those who praised this...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                             review\n",
              "0          1  With all this stuff going down at the moment w...\n",
              "1          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
              "2          0  The film starts with a manager (Nicholas Bell)...\n",
              "3          0  It must be assumed that those who praised this...\n",
              "4          1  Superbly trashy and wondrously unpretentious 8..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg3v7Dk5C8MM",
        "colab_type": "text"
      },
      "source": [
        "# Glove Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2TPQnHh6X0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=train_ds[['review']].copy()\n",
        "y=train_ds[['sentiment']].copy()\n",
        "\n",
        "Xtrain,Xtest,ytrain,ytest=train_test_split(X,y,train_size=0.7,random_state=32,shuffle=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOpARSKe7XDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines=Xtest['review'].apply(lambda x: str(x))\n",
        "\n",
        "lines_list=list([token.lemma_.lower() for token in doc if token.is_alpha and token.is_stop==False] for doc in nlp.pipe(lines, batch_size=1000,disable=['tagger','parser','ner']))\n",
        "\n",
        "lines_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poBl_2-VEEtA",
        "colab_type": "text"
      },
      "source": [
        "# How to use standard Glove vectors for finding similarities without Gensim\n",
        "\n",
        "To load the pre-trained vectors, we must first create a dictionary that will hold the mappings between words, and the embedding vectors of those words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ly5NzFHDi9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "GLOVE_DIR ='/content/drive/My Drive/Python'\n",
        "print('Indexing word vectors.')\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt'),encoding=\"utf8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "#print(embeddings_index['banana'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNhaoY-9uWIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import spatial\n",
        "def find_closest_embeddings(embedding):\n",
        "    return sorted(embeddings_index.keys(), key=lambda word: spatial.distance.euclidean(embeddings_index[word], embedding))\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAdh10DauWjN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "0fe52a00-7211-4237-f4f9-181ccc744ea2"
      },
      "source": [
        "find_closest_embeddings(embeddings_index[\"japanese\"])[:10]"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['japanese',\n",
              " 'japan',\n",
              " 'tokyo',\n",
              " 'korean',\n",
              " 'chinese',\n",
              " 'taiwanese',\n",
              " 'kyodo',\n",
              " 'hashimoto',\n",
              " 'moreover',\n",
              " 'meanwhile']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOQYadkz4dWD",
        "colab_type": "text"
      },
      "source": [
        "# Converting  Standard Glove vectors into Gensim- Word2Vec format for finding similarities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKOunvXkGgtm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "a0134088-3583-40eb-ca87-0ee1e7a2ff4a"
      },
      "source": [
        "!pip3 install glove_python\n",
        "#https://medium.com/analytics-vidhya/word-vectorization-using-glove-76919685ee0b\n",
        "#https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting glove_python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/79/7e7e548dd9dcb741935d031117f4bed133276c2a047aadad42f1552d1771/glove_python-0.1.0.tar.gz (263kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 17.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███▊                            | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 40kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 245kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 256kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 266kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.4.1)\n",
            "Building wheels for collected packages: glove-python\n",
            "  Building wheel for glove-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for glove-python: filename=glove_python-0.1.0-cp36-cp36m-linux_x86_64.whl size=700256 sha256=59040056b7f2464391c77d5e211f15d479ea812527654a2ccc6c39b6000a923f\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/4b/6d/10c0d2ad32c9d9d68beec9694a6f0b6e83ab1662a90a089a4b\n",
            "Successfully built glove-python\n",
            "Installing collected packages: glove-python\n",
            "Successfully installed glove-python-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT9tced9mU7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8bef0cb4-99b7-4a02-8a92-ed91c46e639d"
      },
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove2word2vec(glove_input_file='/content/drive/My Drive/Python/glove.6B.300d.txt', word2vec_output_file=\"/content/drive/My Drive/Python/gensim_glove_vectors.txt\")    \n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "model = KeyedVectors.load_word2vec_format(\"/content/drive/My Drive/Python/gensim_glove_vectors.txt\", binary=False)\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wj5nx9PnEF-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "cb4d762b-d1d7-43eb-a21c-d26ceccdd470"
      },
      "source": [
        "\n",
        "model.most_similar('japanese')\n",
        "\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('japan', 0.769140362739563),\n",
              " ('tokyo', 0.6243131160736084),\n",
              " ('korean', 0.6134214401245117),\n",
              " ('chinese', 0.5663821697235107),\n",
              " ('yen', 0.4867936968803406),\n",
              " ('taiwanese', 0.4863933324813843),\n",
              " ('hashimoto', 0.46149420738220215),\n",
              " ('asian', 0.45883551239967346),\n",
              " ('kyodo', 0.45841461420059204),\n",
              " ('foreign', 0.4445464611053467)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOXJSuk4hY70",
        "colab_type": "text"
      },
      "source": [
        "# Train Glove model on own corpus, find similarities and save the model\n",
        "\n",
        "https://github.com/alexeygrigorev/avito-duplicates-kaggle/blob/master/prepare_glove_model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCohbt5NYke7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "\n",
        "def train_glove(sentences):\n",
        "    print ('training glove model...')\n",
        "    t0 = time()\n",
        "    \n",
        "    num_features = 300    # Word vector dimensionality\n",
        "    context = 5          # Context window size\n",
        "    learning_rate = 0.05\n",
        "    \n",
        "    corpus = Corpus()\n",
        "    corpus.fit(sentences, window=context)\n",
        "\n",
        "    glove = Glove(no_components=num_features, learning_rate=learning_rate)\n",
        "    glove.fit(corpus.matrix, epochs=30, no_threads=8, verbose=True)\n",
        "    glove.add_dictionary(corpus.dictionary)\n",
        "\n",
        "    print('took %0.5fs.' % (time() - t0))\n",
        "    return (glove)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iklmg3QPYnXk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "7bfe0343-bd11-49c0-fcce-19116b5a1b10"
      },
      "source": [
        "gl_model=train_glove(lines_list)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training glove model...\n",
            "Performing 30 training epochs with 8 threads\n",
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n",
            "Epoch 11\n",
            "Epoch 12\n",
            "Epoch 13\n",
            "Epoch 14\n",
            "Epoch 15\n",
            "Epoch 16\n",
            "Epoch 17\n",
            "Epoch 18\n",
            "Epoch 19\n",
            "Epoch 20\n",
            "Epoch 21\n",
            "Epoch 22\n",
            "Epoch 23\n",
            "Epoch 24\n",
            "Epoch 25\n",
            "Epoch 26\n",
            "Epoch 27\n",
            "Epoch 28\n",
            "Epoch 29\n",
            "took 171.18543s.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfVlPCEXbfkk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b365687e-2b22-403b-f027-fe32da398b36"
      },
      "source": [
        "gl_model.most_similar('cast',number=6)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('support', 0.9472058752096072),\n",
              " ('crow', 0.9320960528668345),\n",
              " ('perfectly', 0.9217136045605772),\n",
              " ('ensemble', 0.9155379812714399),\n",
              " ('rest', 0.9066026077791719)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li9x8hvvgeKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gl_model.save('glove_final')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGofyKp1g1nI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rmodel=gl_model.load('glove_final')"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtpjiiRXhBvo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f8ad274c-99c4-451c-a116-79f7191eb2d1"
      },
      "source": [
        "rmodel.most_similar('cast',number=6)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('supporting', 0.8433521223781817),\n",
              " ('excellent', 0.8331830963459859),\n",
              " ('fine', 0.809570443868265),\n",
              " ('perfectly', 0.7971040169644816),\n",
              " ('great', 0.7901401985065863)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    }
  ]
}